{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 Data Cleaning and NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction import stop_words \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data \n",
    "campfire = pd.read_csv('../data/campfire_data.csv')\n",
    "carrfire = pd.read_csv('../data/carrfire_data.csv')\n",
    "hurricane = pd.read_csv('../data/hurricane_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove iirlated column in campfire\n",
    "campfire.drop(columns='Unnamed: 0', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>latitute</th>\n",
       "      <th>longitute</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wed Jan 30 23:59:00 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @ActionNewsNow: The Butte Strong Fund will ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wed Jan 30 23:55:51 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>['PGE']</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @mgafni: Talk over #PGE replicating San Die...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Wed Jan 30 23:52:52 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Federal judge asks PG&amp;amp;E: Should I 'let you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wed Jan 30 23:51:19 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @pbartolone: Will it get harder and harder ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wed Jan 30 23:50:17 +0000 2019</td>\n",
       "      <td>0</td>\n",
       "      <td>['CampFire', 'CampFirePets', 'Paradise', 'Para...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @BCWildfireToday: LOOKING FOR A LOST #CampF...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at  favorite_count  \\\n",
       "0  Wed Jan 30 23:59:00 +0000 2019               0   \n",
       "1  Wed Jan 30 23:55:51 +0000 2019               0   \n",
       "2  Wed Jan 30 23:52:52 +0000 2019               0   \n",
       "3  Wed Jan 30 23:51:19 +0000 2019               0   \n",
       "4  Wed Jan 30 23:50:17 +0000 2019               0   \n",
       "\n",
       "                                            hashtags  latitute  longitute  \\\n",
       "0                                                 []       NaN        NaN   \n",
       "1                                            ['PGE']       NaN        NaN   \n",
       "2                                                 []       NaN        NaN   \n",
       "3                                                 []       NaN        NaN   \n",
       "4  ['CampFire', 'CampFirePets', 'Paradise', 'Para...       NaN        NaN   \n",
       "\n",
       "   retweet_count                                               text  \n",
       "0              0  RT @ActionNewsNow: The Butte Strong Fund will ...  \n",
       "1              0  RT @mgafni: Talk over #PGE replicating San Die...  \n",
       "2              0  Federal judge asks PG&amp;E: Should I 'let you...  \n",
       "3              0  RT @pbartolone: Will it get harder and harder ...  \n",
       "4              0  RT @BCWildfireToday: LOOKING FOR A LOST #CampF...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine all disasters together\n",
    "disaster = pd.concat([campfire, carrfire, hurricane])\n",
    "disaster.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows based on text\n",
    "disaster.drop_duplicates(subset =\"text\", keep = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2384"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2384 unique tweets\n",
    "len(disaster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'disaster' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "#store deduped rows as a dataframe\n",
    "%store disaster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull words in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regularexpression to do lemmatize\n",
    "# use countvectorizer to tokenize, lemmatize, and exclude stopwords \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokenizer = RegexpTokenizer('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "        return [self.wnl.lemmatize(t, 'v') for t in tokenizer.tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# pull words using countvectorizer\n",
    "vectorizer = CountVectorizer(tokenizer = LemmaTokenizer(),\n",
    "                            preprocessor = None,\n",
    "                            stop_words = 'english',\n",
    "                            max_features = 1500,\n",
    "                            ngram_range= (1,2),\n",
    "                            analyzer = 'word', \n",
    "                            min_df=3) \n",
    "\n",
    "disaster_words=disaster['text']\n",
    "disaster_words = vectorizer.fit_transform(disaster_words)\n",
    "# convert into a dataframe\n",
    "disaster_words= pd.DataFrame(disaster_words.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'disaster_words' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "#store words after countvectorizer as a dataframe\n",
    "%store disaster_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count Word Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check words and word_count as a reference for us to create disaster related corpus\n",
    "disaster_word_count = pd.DataFrame(disaster_words.sum(), index=vectorizer.get_feature_names(), columns=['word_count']).sort_values(by='word_count', ascending=False)\n",
    "disaster_word_count = disaster_word_count.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some useless word\n",
    "self_defined_stop_words = ['https', 'rt', 'amp','hurricaneharvey https','link','bio','prop','cosplay',\n",
    "                           'prop cosplay','coffee','jewelry','anime','cosplay anime','coffee prop',\n",
    "                           'victim jewelry','jewelry coffee','propmaster','anime propmaster',\n",
    "                           'beer','ebay','ebay link','link bio','bio carrfire', 'propmaster beer',\n",
    "                           'people','today','carrfire https','make','wine','just','beer wine','come',\n",
    "                           'click','like','work','link carrfire','wine dab','dab weed','weed','look'\n",
    "                          ]\n",
    "                           \n",
    "\n",
    "disaster_word_count = disaster_word_count[~disaster_word_count['index'].isin(self_defined_stop_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed some top unrelated words\n",
    "# export to csv for a clear view of related words\n",
    "disaster_word_count.to_csv('../data/tweets_word_count.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
