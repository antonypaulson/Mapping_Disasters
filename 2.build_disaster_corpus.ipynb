{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build Disaster Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a disaster corpus using related words in training set and word2vec."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Data Cleaning and NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from sklearn.feature_extraction import stop_words \n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data \n",
    "campfire = pd.read_csv('./data/campfire_tweets.csv')\n",
    "carrfire = pd.read_csv('./data/carrfire_tweets.csv')\n",
    "hurricane = pd.read_csv('./data/hurricane_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove iirlated column in hurricane\n",
    "hurricane.drop(columns=['latitute','longitute'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Thu Nov 08 23:59:59 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>['CampFire']</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @RealJamesWoods: Any info? Call 435-238-073...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Thu Nov 08 23:59:59 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>['CampFire']</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @RealJamesWoods: IMPORTANT!!! 911 overwhelm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Thu Nov 08 23:59:59 +0000 2018</td>\n",
       "      <td>10</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>Think it’s getting to the point where it may b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Thu Nov 08 23:59:59 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>['BREAKING', 'CampFire']</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @nbcbayarea: #BREAKING: Fast-moving #CampFi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thu Nov 08 23:59:58 +0000 2018</td>\n",
       "      <td>0</td>\n",
       "      <td>['CampFire']</td>\n",
       "      <td>0</td>\n",
       "      <td>RT @DaniD0909: My BIL was stuck in a safe zone...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       created_at  favorite_count                  hashtags  \\\n",
       "0  Thu Nov 08 23:59:59 +0000 2018               0              ['CampFire']   \n",
       "1  Thu Nov 08 23:59:59 +0000 2018               0              ['CampFire']   \n",
       "2  Thu Nov 08 23:59:59 +0000 2018              10                        []   \n",
       "3  Thu Nov 08 23:59:59 +0000 2018               0  ['BREAKING', 'CampFire']   \n",
       "4  Thu Nov 08 23:59:58 +0000 2018               0              ['CampFire']   \n",
       "\n",
       "   retweet_count                                               text  \n",
       "0              0  RT @RealJamesWoods: Any info? Call 435-238-073...  \n",
       "1              0  RT @RealJamesWoods: IMPORTANT!!! 911 overwhelm...  \n",
       "2              0  Think it’s getting to the point where it may b...  \n",
       "3              0  RT @nbcbayarea: #BREAKING: Fast-moving #CampFi...  \n",
       "4              0  RT @DaniD0909: My BIL was stuck in a safe zone...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine all three files\n",
    "disaster_tweets = pd.concat([campfire, carrfire, hurricane])\n",
    "disaster_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6396, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "disaster_tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate rows based on text\n",
    "disaster_tweets.drop_duplicates(subset =\"text\", keep = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1397"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1397 unique tweets\n",
    "len(disaster_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'disaster_tweets' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# store the data for further use\n",
    "%store disaster_tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pull words in tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regularexpression to do lemmatize\n",
    "# use countvectorizer to tokenize, lemmatize, and exclude stopwords \n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokenizer = RegexpTokenizer('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "        return [self.wnl.lemmatize(t, 'v') for t in tokenizer.tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# pull words using countvectorizer\n",
    "vectorizer = CountVectorizer(tokenizer = LemmaTokenizer(),\n",
    "                            preprocessor = None,\n",
    "                            stop_words = 'english',\n",
    "                            max_features = 1500,\n",
    "                            ngram_range= (1,2),\n",
    "                            analyzer = 'word', \n",
    "                            min_df=1) \n",
    "\n",
    "tweets_words= disaster_tweets['text']\n",
    "tweets_words = vectorizer.fit_transform(tweets_words)\n",
    "# convert into a dataframe\n",
    "tweets_words= pd.DataFrame(tweets_words.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'tweets_words' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# store the data for EDA\n",
    "%store tweets_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count word frequency as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check words and word_count as a reference for us to create disaster related corpus\n",
    "tweets_word_count = pd.DataFrame(tweets_words.sum(), index=vectorizer.get_feature_names(), columns=['word_count']).sort_values(by='word_count', ascending=False)\n",
    "tweets_word_count = tweets_word_count.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export to csv for a clear view of related words\n",
    "tweets_word_count.to_csv('./data/tweets_word_count.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove word that doesn't make sense or completely useless\n",
    "self_defined_stop_words = ['https', 'rt', 'amp','hurricaneharvey https','299','rd','hwy','hwy 299',\n",
    "                           'powerhouse rd','000', '0','just','299 carr','sr','200 amp','299w','tx',\n",
    "                           'sr 299w', 'htt', 'gt', 've', 'll', 'jjwatt', 'st','ht','ho','nra','th',\n",
    "                           'lt','gt gt', '500 500']\n",
    "                           \n",
    "\n",
    "tweets_word_count = tweets_word_count[~tweets_word_count['index'].isin(self_defined_stop_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'self_defined_stop_words' (list)\n"
     ]
    }
   ],
   "source": [
    "%store self_defined_stop_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Build Disaster Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hurricaneharvey</td>\n",
       "      <td>851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>carrfire</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>help</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>campfire</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>houston</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             index  word_count\n",
       "1  hurricaneharvey         851\n",
       "3         carrfire         238\n",
       "4             help         212\n",
       "7         campfire         138\n",
       "8          houston         128"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets_word_count after removing self_defined_stop_words\n",
    "tweets_word_count.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of words related to disasters from tweets_word_count\n",
    "tweets_word_count_list = [row.split(',') for row in tweets_word_count['index']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check word frequency as a reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Word Frequency as a reference to build a basic help_words list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# picked up words based on tweets_count_word\n",
    "# some interesting findings: 'please help' and 'pleasehelp' both exists\n",
    " \n",
    "help_words = [# words in tweets_count_word\n",
    "              'help', 'victims', 'flood', 'donations', 'donate', 'shelter','redcross',\n",
    "              # self-added words\n",
    "              'please help','pleasehelp','need help','needhelp','assistance',\n",
    "              'support','need support','lose home','red across', 'lost',\n",
    "              'dab','debris','disaster','devastate','damage','emergency','destroy','disaster assist team',\n",
    "              'support harricane','disasterassistteam','severe','unsafe',\n",
    "              'need shelter', 'help us', 'help me', 'flooding']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create disaster corpus  based on exsiting tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use help_words as above to roughly find all related words in the existing tweets. This corpus is used to label our training data(existing tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Word2Vec\n",
    "import numpy as np\n",
    "import time\n",
    "import gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Import Word2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# If you want to use gensim's data, import their downloader\n",
    "# and load it.\n",
    "import gensim.downloader as api\n",
    "corpus = tweets_word_count_list\n",
    "\n",
    "# If you have your own iterable corpus of cleaned data:\n",
    "\n",
    "# Train a model! \n",
    "model = Word2Vec(corpus,      # Corpus of data.\n",
    "                 size=100,    # How many dimensions do you want in your word vector?\n",
    "                 window=5,    # How many \"context words\" do you want?\n",
    "                 min_count=1, # Ignores words below this threshold.\n",
    "                 sg=0,        # SG = 1 uses SkipGram, SG = 0 uses CBOW (default).\n",
    "                 workers=4)   # Number of \"worker threads\" to use (parallelizes process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "similar_words = []\n",
    "for words in help_words:\n",
    "    try:\n",
    "        similar_words.append(list(zip(*model.most_similar(f\"{words}\")))[0])\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# each element in similar_words is a tuple\n",
    "# change each element as a word\n",
    "similar_words = [i for sub in similar_words for i in sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "145"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# got 138 similar words\n",
    "len(set(similar_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is our final tweets_disaster_corpus\n",
    "# hand picked help_words + similar_words extracted from training set\n",
    "tweets_disaster_corpus = help_words + similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tweets_disaster_corpus includes 169 related words\n",
    "tweets_disaster_corpus = list(set(tweets_disaster_corpus))\n",
    "len(tweets_disaster_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create disaster corpus based on Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This corpus is an extension of the previous existing disaster corpus. This new disaster corpus is used to predict whether a new tweet is related to diasater. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Word2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "# If you want to use gensim's data, import their downloader\n",
    "# and load it.\n",
    "import gensim.downloader as api\n",
    "corpus = api.load('text8')\n",
    "\n",
    "# If you have your own iterable corpus of cleaned data:\n",
    "\n",
    "# Train a model! \n",
    "model = Word2Vec(corpus,      # Corpus of data.\n",
    "                 size=100,    # How many dimensions do you want in your word vector?\n",
    "                 window=5,    # How many \"context words\" do you want?\n",
    "                 min_count=1, # Ignores words below this threshold.\n",
    "                 sg=0,        # SG = 1 uses SkipGram, SG = 0 uses CBOW (default).\n",
    "                 workers=4)   # Number of \"worker threads\" to use (parallelizes process).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "wiki_similar_words = []\n",
    "for words in tweets_disaster_corpus:\n",
    "    try:\n",
    "        wiki_similar_words.append(list(zip(*model.most_similar(f\"{words}\")))[0])\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change each element as a word\n",
    "wiki_similar_words = [i for sub in wiki_similar_words for i in sub]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "859"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wiki_similar_words includes 839 related words\n",
    "wiki_similar_words = list(set(wiki_similar_words))\n",
    "len(wiki_similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand picked related words in wiki_similar_words\n",
    "wiki_similar_words = [\n",
    " 'disaster',\n",
    " 'relief',\n",
    " 'accidents',\n",
    " 'deficit',\n",
    " 'risks',\n",
    " 'devour',\n",
    " 'inflict',\n",
    " 'rehabilitation',\n",
    " 'cessation',\n",
    " 'diagnose',\n",
    " 'migrate',\n",
    " 'agencies',\n",
    " 'needs',\n",
    " 'christ',\n",
    " 'subdue',\n",
    " 'disasters',\n",
    " 'concerns',\n",
    " 'defend',\n",
    " 'uncertainty',\n",
    " 'resettle',\n",
    " 'clinical',\n",
    " 'necessity',\n",
    " 'confront',\n",
    " 'inflation',\n",
    " 'prolonged',\n",
    " 'tunnel',\n",
    " 'organize',\n",
    " 'liberate',\n",
    " 'assemble',\n",
    " 'assistance',\n",
    " 'needed',\n",
    " 'flee',\n",
    " 'fear',\n",
    " 'crises',\n",
    " 'pull',\n",
    " 'accident',\n",
    " 'navigate',\n",
    " 'hide',\n",
    " 'marshes',\n",
    " 'secured',\n",
    " 'storms',\n",
    " 'danger',\n",
    " 'responses',\n",
    " 'hurricane',\n",
    " 'leave',\n",
    " 'sinking',\n",
    " 'try',\n",
    " 'destroy',\n",
    " 'aid',\n",
    " 'situation',\n",
    " 'abandoned',\n",
    " 'explosion',\n",
    " 'rejoin',\n",
    " 'canal',\n",
    " 'borrowing',\n",
    " 'breath',\n",
    " 'alluvium',\n",
    " 'crisis',\n",
    " 'abandonment',\n",
    " 'flooding',\n",
    " 'vulnerability',\n",
    " 'surgical',\n",
    " 'hurricanes',\n",
    " 'assist',\n",
    " 'believer',\n",
    " 'seek',\n",
    " 'hot',\n",
    " 'tornadoes',\n",
    " 'strive',\n",
    " 'communicate',\n",
    " 'raise',\n",
    " 'climb',\n",
    " 'regained',\n",
    " 'preventing',\n",
    " 'safety',\n",
    " 'god',\n",
    " 'flood',\n",
    " 'survivors',\n",
    " 'weigh',\n",
    " 'loss',\n",
    " 'explodes',\n",
    " 'lava',\n",
    " 'droughts',\n",
    " 'send',\n",
    " 'tsunami',\n",
    " 'escape',\n",
    " 'shelters',\n",
    " 'defeated',\n",
    " 'rains',\n",
    " 'precipitation',\n",
    " 'withdrawal',\n",
    " 'darkest',\n",
    " 'killings',\n",
    " 'injure',\n",
    " 'dust',\n",
    " 'shortage',\n",
    " 'snowfall',\n",
    " 'winds',\n",
    " 'foodstocks',\n",
    " 'sediment',\n",
    "    'sediments',\n",
    " 'prayers',\n",
    " 'debris',\n",
    " 'procure',\n",
    " 'smoke',\n",
    " 'respond',\n",
    " 'floods',\n",
    " 'injuries',\n",
    " 'lose',\n",
    " 'distribute',\n",
    " 'devastating',\n",
    " 'allocate',\n",
    " 'helping',\n",
    " 'droplets',\n",
    " 'confine',\n",
    " 'medicine',\n",
    " 'downfall',\n",
    " 'forests',\n",
    " 'recovery',\n",
    " 'fires',\n",
    " 'explosions',\n",
    " 'earthquakes',\n",
    " 'alienate',\n",
    " 'lightning',\n",
    " 'silt',\n",
    " 'needing',\n",
    " 'rainforest',\n",
    " 'katrina',\n",
    " 'hypothermia',\n",
    " 'responding',\n",
    " 'toxin',\n",
    " 'erosion',\n",
    " 'swamps',\n",
    " 'eruption',\n",
    " 'relocate',\n",
    " 'starvation',\n",
    " 'losing',\n",
    " 'abandon',\n",
    " 'collect',\n",
    " 'freshwater',\n",
    " 'kill',\n",
    " 'detention',\n",
    " 'fund',\n",
    " 'fatal',\n",
    " 'hazards',\n",
    " 'damaging',\n",
    " 'queue',\n",
    " 'regain',\n",
    " 'health',\n",
    " 'rain',\n",
    " 'teams',\n",
    " 'casualties',\n",
    " 'protection',\n",
    " 'reservoir',\n",
    " 'ports',\n",
    " 'supplies',\n",
    " 'miracle',\n",
    " 'accommodate',\n",
    " 'freezing',\n",
    " 'salvation',\n",
    " 'deterioration',\n",
    " 'locate',\n",
    " 'catastrophic',\n",
    " 'physicians',\n",
    " 'injury',\n",
    " 'harbour',\n",
    " 'emergency',\n",
    " 'humanitarian',\n",
    " 'breach',\n",
    " 'painful',\n",
    " 'soils',\n",
    " 'storm',\n",
    " 'serious']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine two corpus\n",
    "tweets_and_wiki_corpus = wiki_similar_words + tweets_disaster_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "337"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove duplicate words\n",
    "# get 331 words that really related to disaster\n",
    "tweets_and_wiki_corpus = list(set(tweets_and_wiki_corpus))\n",
    "len(tweets_and_wiki_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hand picked final disaster words for labeling\n",
    "final_disaster_words = ['needing',\n",
    " 'danger',\n",
    " 'disaster',\n",
    " 'hypothermia',\n",
    " 'injure',\n",
    " 'victim',\n",
    " 'sinking',\n",
    " 'damage',\n",
    " 'support harricane',\n",
    " 'accidents',\n",
    " 'destroy',\n",
    " 'aid',\n",
    " 'toxin',\n",
    " 'erosion',\n",
    " 'redcross',\n",
    " 'abandoned',\n",
    " 'evacuate',\n",
    " 'disaster assist team',\n",
    " 'explosion',\n",
    " 'victims',\n",
    " 'crisis',\n",
    " 'abandonment',\n",
    " 'need shelter',\n",
    " 'help me',\n",
    " 'starvation',\n",
    " 'flooding',\n",
    " 'losing',\n",
    " 'disasterassistteam',\n",
    " 'needhelp',\n",
    " 'foodstocks',\n",
    " 'assist',\n",
    " 'abandon',\n",
    " 'sediment',\n",
    " 'destroy home',\n",
    " 'donate',\n",
    " 'sediments',\n",
    " 'needs',\n",
    " 'disasters',\n",
    " 'help us',\n",
    " 'property',\n",
    " 'fatal',\n",
    " 'debris',\n",
    " 'medical',\n",
    " 'hazards',\n",
    " 'damaging',\n",
    " 'smoke',\n",
    " 'health',\n",
    " 'pleasehelp',\n",
    " 'flood',\n",
    " 'casualties',\n",
    " 'respond',\n",
    " 'survivors',\n",
    " 'donations',\n",
    " 'floods',\n",
    " 'loss',\n",
    " 'injuries',\n",
    " 'explodes',\n",
    " 'need support',\n",
    " 'assistance',\n",
    " 'supplies',\n",
    " 'needed',\n",
    " 'lose',\n",
    " 'lose home',\n",
    " 'devastate',\n",
    " 'fear',\n",
    " 'crises',\n",
    " 'need help',\n",
    " 'droughts',\n",
    " 'send help',\n",
    " 'unsafe',\n",
    " 'death',\n",
    " 'freezing',\n",
    " 'severe',\n",
    " 'devastating',\n",
    " 'catastrophic',\n",
    " 'please help',\n",
    " 'accident',\n",
    " 'helping',\n",
    " 'shelter',\n",
    " 'escape',\n",
    " 'shelters',\n",
    " 'navigate',\n",
    " 'tragedy',\n",
    " 'injury',\n",
    " 'medicine',\n",
    " 'emergency',\n",
    " 'recovery',\n",
    " 'fires',\n",
    " 'explosions',\n",
    " 'breach',\n",
    " 'help',\n",
    " 'flame',\n",
    " 'painful',\n",
    " 'lightning',\n",
    " 'withdrawal',\n",
    " 'storm',\n",
    " 'storms',\n",
    " 'lost',\n",
    " 'help needed',\n",
    " 'serious']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(final_disaster_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'final_disaster_words' (list)\n"
     ]
    }
   ],
   "source": [
    "%store final_disaster_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
