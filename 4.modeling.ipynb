{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Label tweets as 0 and 1 based on key words to train classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Label tweets based on key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "%store -r disaster_tweets\n",
    "%store -r self_defined_stop_words\n",
    "%store -r final_disaster_words\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import VotingClassifier, RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.feature_extraction import stop_words \n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB, ComplementNB\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.metrics import mean_squared_error, f1_score, confusion_matrix\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_tweets.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets related to disaster is 942\n",
      "Number of overall tweets is 1397\n"
     ]
    }
   ],
   "source": [
    "# we tried self defined words to label tweets as well\n",
    "# but our final_disaster_words list works better\n",
    "\n",
    "# words=[\n",
    "#  'need shelter',\n",
    "#  'help me',\n",
    "#  'needhelp',\n",
    "#  'help us',\n",
    "#  'pleasehelp',\n",
    "#  'casualties',\n",
    "#  'assistance',\n",
    "#  'need help',\n",
    "#  'send help',\n",
    "#  'please help',\n",
    "#  'shelter',\n",
    "#  'emergency']\n",
    "\n",
    "words = final_disaster_words\n",
    "\n",
    "disaster_tweet_index = []\n",
    "words = final_disaster_words\n",
    "\n",
    "for i in words:\n",
    "    for x in range(len(disaster_tweets['text'])):\n",
    "        if i in disaster_tweets['text'][x]:\n",
    "            disaster_tweet_index.append(x)\n",
    "print(f'Number of tweets related to disaster is {len(disaster_tweet_index)}')\n",
    "print(f'Number of overall tweets is {len(disaster_tweets)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# label tweets as 0 or 1\n",
    "disaster_tweets['label'] = 0\n",
    "disaster_tweets['label'][disaster_tweet_index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Build models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 834), (1, 834)]\n"
     ]
    }
   ],
   "source": [
    "X = disaster_tweets['text']\n",
    "y = disaster_tweets['label']\n",
    "\n",
    "# resampling to create a balanced class\n",
    "ros = RandomOverSampler(random_state=0) # Instantiate a random oversampler in order to oversample our training set\n",
    "X_resampled, y_resampled = ros.fit_resample(X.values.reshape(-1, 1), y) # Fit that oversampler to our X_sc (scaled) and y data\n",
    "print(sorted(Counter(y_resampled).items())) # Show the balance btw classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test data split in training set\n",
    "# test data here is used to evaluate\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count vectorizer words\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        tokenizer = RegexpTokenizer('(?u)\\\\b\\\\w\\\\w+\\\\b')\n",
    "        return [self.wnl.lemmatize(t, 'v') for t in tokenizer.tokenize(doc)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['200', '500', 'carr', 'hurricaneharvey', 'make', 'powerhouse'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text \n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(self_defined_stop_words)\n",
    "vectorizer = CountVectorizer(tokenizer = LemmaTokenizer(),\n",
    "                            preprocessor = None,\n",
    "                            stop_words = stop_words,\n",
    "                            max_features = 1500,\n",
    "                            ngram_range= (1,2),\n",
    "                            analyzer = 'word', \n",
    "                            min_df=3) \n",
    "vectorizer.fit(X_train.ravel())\n",
    "X_train = vectorizer.transform(X_train.ravel())\n",
    "X_test = vectorizer.transform(X_test.ravel())\n",
    "X_train_df = pd.DataFrame(X_train.toarray(), columns=vectorizer.get_feature_names())\n",
    "X_test_df = pd.DataFrame(X_test.toarray(), columns=vectorizer.get_feature_names())\n",
    "y_train_df = pd.DataFrame(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best params to build classification model\n",
    "lr_class = LogisticRegression(penalty='l1', C=40, solver='liblinear')\n",
    "# knn_class = KNeighborsClassifier(n_neighbors=3, p=4, leaf_size=10, metric='minkowski')\n",
    "tree_class = DecisionTreeClassifier(max_features='auto', min_samples_leaf=3, min_samples_split=4, random_state=100)\n",
    "bag_class = BaggingClassifier(bootstrap=False, max_features=8, max_samples=100, n_estimators=100, random_state=100)\n",
    "forest_class = RandomForestClassifier(bootstrap=True, max_leaf_nodes=None, min_samples_leaf=3, min_samples_split=8, n_estimators=9, random_state=100)\n",
    "ada_class = AdaBoostClassifier(learning_rate=0.78, n_estimators=100, random_state=100)\n",
    "svc = SVC(degree=8, C=2.5, gamma=0.1, kernel='poly', random_state=100)\n",
    "grad_class = GradientBoostingClassifier(learning_rate=0.06464371632490062, n_estimators=100, min_samples_leaf=2, min_samples_split=5, max_depth=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use below code to find the best params for each model\n",
    "# params = {\n",
    "#     'max_depth' : [5, 6, 7],\n",
    "#     'min_samples_leaf' : [2, 3],\n",
    "#     'min_samples_split' : [4, 5, 6],\n",
    "#     'n_estimators' : [50, 100, 125],\n",
    "#     \"learning_rate\" : (np.logspace(-1.6, -1, 20))\n",
    "# }\n",
    "# gs = GridSearchCV(\n",
    "#     GradientBoostingClassifier(),\n",
    "#     params,\n",
    "#     cv=3,\n",
    "#     verbose=1,\n",
    "#     return_train_score=False,\n",
    "#     n_jobs=2)\n",
    "# gs.fit(X_train, y_train)\n",
    "# print(gs.best_score_)\n",
    "# print()\n",
    "# print(gs.best_params_)\n",
    "# print()\n",
    "# print(gs.score(X_test, y_test))\n",
    "# pred = model.predict(X_test)\n",
    "# f1_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_models = {\n",
    "            'lr_class': lr_class, \n",
    "            'forest_class': forest_class, \n",
    "            'tree_class': tree_class,\n",
    "            'ada_class': ada_class,            \n",
    "            'bag_class': bag_class, \n",
    "            'svc': svc,\n",
    "            \"grad\": grad_class\n",
    "                } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(417, 7)\n",
      "(1251, 7)\n"
     ]
    }
   ],
   "source": [
    "y_pred_testc = []\n",
    "y_pred_trainc = []\n",
    "\n",
    "for model in class_models.values():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred_testc.append(model.predict(X_test))\n",
    "    y_pred_trainc.append(model.predict(X_train))\n",
    "\n",
    "y_pred_testc_df = pd.DataFrame(y_pred_testc, index=class_models.keys()).T\n",
    "y_pred_trainc_df = pd.DataFrame(y_pred_trainc, index=class_models.keys()).T\n",
    "print(y_pred_testc_df.shape)\n",
    "print(y_pred_trainc_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>F1-train</th>\n",
       "      <th>F1-test</th>\n",
       "      <th>true_neg</th>\n",
       "      <th>fal_pos</th>\n",
       "      <th>fal_neg</th>\n",
       "      <th>true_po</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lr_class</th>\n",
       "      <td>0.998401</td>\n",
       "      <td>0.868106</td>\n",
       "      <td>0.998379</td>\n",
       "      <td>0.874715</td>\n",
       "      <td>170</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forest_class</th>\n",
       "      <td>0.901679</td>\n",
       "      <td>0.846523</td>\n",
       "      <td>0.900566</td>\n",
       "      <td>0.848341</td>\n",
       "      <td>174</td>\n",
       "      <td>27</td>\n",
       "      <td>37</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tree_class</th>\n",
       "      <td>0.884093</td>\n",
       "      <td>0.832134</td>\n",
       "      <td>0.886807</td>\n",
       "      <td>0.843750</td>\n",
       "      <td>158</td>\n",
       "      <td>43</td>\n",
       "      <td>27</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada_class</th>\n",
       "      <td>0.926459</td>\n",
       "      <td>0.846523</td>\n",
       "      <td>0.924466</td>\n",
       "      <td>0.843137</td>\n",
       "      <td>181</td>\n",
       "      <td>20</td>\n",
       "      <td>44</td>\n",
       "      <td>172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bag_class</th>\n",
       "      <td>0.599520</td>\n",
       "      <td>0.522782</td>\n",
       "      <td>0.332889</td>\n",
       "      <td>0.167364</td>\n",
       "      <td>198</td>\n",
       "      <td>3</td>\n",
       "      <td>196</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svc</th>\n",
       "      <td>0.808153</td>\n",
       "      <td>0.633094</td>\n",
       "      <td>0.759036</td>\n",
       "      <td>0.451613</td>\n",
       "      <td>201</td>\n",
       "      <td>0</td>\n",
       "      <td>153</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>grad</th>\n",
       "      <td>0.948841</td>\n",
       "      <td>0.880096</td>\n",
       "      <td>0.947282</td>\n",
       "      <td>0.881517</td>\n",
       "      <td>181</td>\n",
       "      <td>20</td>\n",
       "      <td>30</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 train      test  F1-train   F1-test  true_neg  fal_pos  \\\n",
       "lr_class      0.998401  0.868106  0.998379  0.874715       170       31   \n",
       "forest_class  0.901679  0.846523  0.900566  0.848341       174       27   \n",
       "tree_class    0.884093  0.832134  0.886807  0.843750       158       43   \n",
       "ada_class     0.926459  0.846523  0.924466  0.843137       181       20   \n",
       "bag_class     0.599520  0.522782  0.332889  0.167364       198        3   \n",
       "svc           0.808153  0.633094  0.759036  0.451613       201        0   \n",
       "grad          0.948841  0.880096  0.947282  0.881517       181       20   \n",
       "\n",
       "              fal_neg  true_po  \n",
       "lr_class           24      192  \n",
       "forest_class       37      179  \n",
       "tree_class         27      189  \n",
       "ada_class          44      172  \n",
       "bag_class         196       20  \n",
       "svc               153       63  \n",
       "grad               30      186  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create summary chart\n",
    "accuracy = {'train': [], 'test': [], 'F1-train': [], 'F1-test': [], 'true_neg': [], 'fal_pos': [], 'fal_neg': [], 'true_po': []}\n",
    "for model in class_models.values():\n",
    "    accuracy['train'].append(model.score(X_train, y_train))\n",
    "    accuracy['test'].append(model.score(X_test, y_test))\n",
    "for col in y_pred_testc_df:\n",
    "    accuracy['F1-train'].append(f1_score(y_train, y_pred_trainc_df[col]))\n",
    "    accuracy['F1-test'].append(f1_score(y_test, y_pred_testc_df[col]))\n",
    "for col in y_pred_testc_df:\n",
    "    accuracy['true_neg'].append(confusion_matrix(y_test, y_pred_testc_df[col])[0][0])\n",
    "    accuracy['fal_pos'].append(confusion_matrix(y_test, y_pred_testc_df[col])[0][1])\n",
    "    accuracy['fal_neg'].append(confusion_matrix(y_test, y_pred_testc_df[col])[1][0])\n",
    "    accuracy['true_po'].append(confusion_matrix(y_test, y_pred_testc_df[col])[1][1])\n",
    "    \n",
    "accuracy_df = pd.DataFrame(accuracy, index=class_models.keys())\n",
    "accuracy_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Apply models on new tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read new tweets\n",
    "campfire_new_tweets = pd.read_csv('./data/campfire_new_tweets.csv')\n",
    "campfire_new_tweets_2 = pd.read_csv('./data/campfire_new_tweets_2.csv')\n",
    "carrfire_new_tweets = pd.read_csv('./data/carrfire_new_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine results\n",
    "new_tweets = pd.concat([campfire_new_tweets,campfire_new_tweets_2,carrfire_new_tweets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove duplicate tweets\n",
    "new_tweets.drop_duplicates(subset =\"text\", keep = False, inplace = True)\n",
    "new_tweets.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "832"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have 832 tweets to predict\n",
    "len(new_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply countvectorizer on X_pred\n",
    "X_pred = new_tweets['text']\n",
    "X_pred = vectorizer.transform(X_pred.ravel())\n",
    "X_pred_df = pd.DataFrame(X_pred.toarray(), columns=vectorizer.get_feature_names())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply models on X_pred_df\n",
    "prediction = pd.DataFrame(index=X_pred_df.index)\n",
    "\n",
    "for (model_name, model) in class_models.items():\n",
    "    prediction[model_name] = model.predict(X_pred_df)\n",
    "prediction['total'] = prediction['lr_class'] + prediction['forest_class'] + prediction['tree_class'] + \\\n",
    "        prediction['ada_class'] + prediction['bag_class'] + prediction['svc'] + prediction['grad'] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lr_class</th>\n",
       "      <th>forest_class</th>\n",
       "      <th>tree_class</th>\n",
       "      <th>ada_class</th>\n",
       "      <th>bag_class</th>\n",
       "      <th>svc</th>\n",
       "      <th>grad</th>\n",
       "      <th>total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     lr_class  forest_class  tree_class  ada_class  bag_class  svc  grad  \\\n",
       "496         1             1           1          1          1    0     1   \n",
       "0           1             1           1          1          0    0     1   \n",
       "130         1             1           1          1          0    0     1   \n",
       "598         1             1           1          1          0    0     1   \n",
       "769         1             1           1          1          0    0     1   \n",
       "1           1             1           1          1          0    0     1   \n",
       "124         1             1           1          1          0    0     1   \n",
       "126         1             1           1          1          0    0     1   \n",
       "127         1             1           1          1          0    0     1   \n",
       "131         1             1           1          1          0    0     1   \n",
       "\n",
       "     total  \n",
       "496      6  \n",
       "0        5  \n",
       "130      5  \n",
       "598      5  \n",
       "769      5  \n",
       "1        5  \n",
       "124      5  \n",
       "126      5  \n",
       "127      5  \n",
       "131      5  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine all predict together\n",
    "prediction.sort_values(by='total', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 1, 2, 4, 3, 6])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number in the array means how many model predicted it as 1\n",
    "prediction['total'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of tweets marked as 1 by 3 or more models\n",
    "len(prediction[~((prediction['total']==0)|(prediction['total']==1)|(prediction['total']==2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index of rows that are predicted as \"related to disaster and need help\"\n",
    "index_true= prediction[~((prediction['total']==0)|(prediction['total']==1)|(prediction['total']==1))].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     Found on Neal please help... Call AC 949680884...\n",
       "1     #HappyHolidays from #ParadiseCalifornia help m...\n",
       "2     We are here to help. #LTMA #LessTalkMoreAction...\n",
       "5     More pics from @viralwebstudio to go with my l...\n",
       "6     Yesterday we returned to Butte County to help ...\n",
       "9     \"Hey. Are all the arrangements made? I think J...\n",
       "11    Huge THANK YOU  to all who #donate #volunteer ...\n",
       "12    Shaila worked nonstop to help organize. Jamie ...\n",
       "14    As many of you have seen and donated to our ef...\n",
       "16    Heartbreak for miles in Chico. Grateful to so ...\n",
       "17    Been working on this non stop so happy it came...\n",
       "22    Sunday evening by the fire. @ Oroville East, C...\n",
       "23    We’re headed to Chico, California tomorrow to ...\n",
       "25    Paradise, California: January 25, 2019 \\nYeste...\n",
       "26    Mod MOVERS is 30 minutes away from what’s refe...\n",
       "28    Such a joy to see a group of @challengecsuc st...\n",
       "37    #Chico!! TONITE 9pm at sierranevadachico big r...\n",
       "39    #CHICO #CA!! TOMORROW 1/13 we rock the sierran...\n",
       "40    #CHICO #CA!! TOMORROW 1/13 we rock the sierran...\n",
       "41    #Chico!! See U this Sunday at sierranevadachic...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# quick check of related tweets\n",
    "new_tweets.iloc[index_true]['text'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull longitute and latitute\n",
    "index_true= index_true.to_list()\n",
    "coord_true = new_tweets.iloc[index_true][['latitute','longitute']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitute</th>\n",
       "      <th>longitute</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.76170</td>\n",
       "      <td>-121.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39.76170</td>\n",
       "      <td>-121.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39.76170</td>\n",
       "      <td>-121.609000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>39.72630</td>\n",
       "      <td>-121.835800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>39.51878</td>\n",
       "      <td>-121.567266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   latitute   longitute\n",
       "0  39.76170 -121.609000\n",
       "1  39.76170 -121.609000\n",
       "2  39.76170 -121.609000\n",
       "5  39.72630 -121.835800\n",
       "6  39.51878 -121.567266"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coord_true.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export coordinates as csv file to plot\n",
    "coord_true.to_csv('./data/coord_to_plot.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
